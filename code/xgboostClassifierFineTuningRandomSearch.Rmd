---
title: 'XGBoost classifier'
author: "joaquin salas"
date: "2020.06.29"
output:
  pdf_document: default
  word_document: default
  
  #data saved in: "featureSelection.RData"
  #save(list = ls(all.names=TRUE), file = "featureSelection.RData")
  
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
Given that a patient has been confirmed positive, 
this program construct an XGBoost classifier to infer whether the patient 
will improve or will die.
To assess the classifier, we construct ROC and precision-recall curves.

https://insightr.wordpress.com/2018/05/17/tuning-xgboost-in-r-part-i/

https://www.r-bloggers.com/tuning-xgboost-in-r-part-ii/


```{r  echo=FALSE}




#import libraries
suppressMessages(library(xgboost))

suppressMessages(library(caret))
suppressMessages(library(caTools))

suppressMessages(library(data.table)) # setnames

suppressMessages(require(dplyr))

suppressMessages(require(dummies)) #dummy variables
suppressMessages(require(ggplot2))
suppressMessages(require("Hmisc"))
suppressMessages(library("installr"))
suppressMessages(library(lubridate, warn.conflict = FALSE, quietly = TRUE))
suppressMessages(library(missForest)) #missForest
suppressMessages(library(matrixStats)) #colSds, standard deviation

suppressMessages(require(multiROC))
suppressMessages(require(nbpMatching))

suppressMessages(require(plotly))
suppressMessages(require(PRROC))
suppressMessages(library(pROC,verbose = TRUE )) #pROC, ROC analysis
suppressMessages(require(plyr))

suppressMessages(suppressWarnings(library(randomForest))) #for randomForest
suppressMessages(library(readr))

suppressMessages(library(stringi))

suppressMessages(library(tidyr)) #fill


suppressMessages(library(e1071))

suppressMessages(library(kernlab)) #svm



source("readDataSVM.R")




```


## Read the data

Dado que utilizaremos Boruta para la seleccion de caracteristicas, reducimos la base de datos mediante la siguiente estrategia. 


```{r preparacion, echo=FALSE}


#working directories
code.dir = 'E:/Documents/informs/research/2020.06.25 predicting death/code/'
setwd(code.dir)

#directories with files that  I use across many applications related to COVID
data.dir = 'E://Documents//informs//research//2020.06.25 predicting death//data//'



#files in the data.dir directory corresponding to the Health Ministery data
files.pattern = "a_*CENSO NOMINAL DF*.*"

files <- list.files(path = data.dir, pattern =  files.pattern)

#############

i = 1
for (file in files) {
  
  filename = paste(data.dir, file, sep = "")
  
  dataset = readDataSVM(filename = filename)
  
  if (i == 1) {
    data = dataset
  }
  else {
    data = rbind(data, dataset)
  }
  i = i + 1
}








covid = data[data$DIAGNOSTICO_FINAL == "COVID-19",!(names(data) %in% c("DIAGNOSTICO_FINAL"))]
condicion.medica = (covid$DESC_MOTIVO_EGRESO == "MEJORIA") | 
  (covid$DESC_MOTIVO_EGRESO == "DEFUNCION")

covid.diag = covid[condicion.medica,]

threshold.nan = 0 # the current database is full

covid.diag.pred = covid.diag[, !(names(covid.diag) %in% c("DESC_MOTIVO_EGRESO"))]

#change predictor from factor <MEJORIA, DEFUNCION> to factor <0,1>
pred = covid.diag$DESC_MOTIVO_EGRESO
pred.num = c(0,nrow=length(pred), ncol= 1)
pred.num[pred==c("MEJORIA")] = 0
pred.num[pred==c("DEFUNCION")] = 1


#pred.num = pred.num

too.many.nan = 
  names(covid.diag.pred)[colSums(is.na(covid.diag.pred)) > threshold.nan]


#remove variables for which there are too many missing values
covid.no.nan = covid.diag.pred[, !(names(covid.diag.pred) %in% c(too.many.nan))]




##fill missing values
covid.imp = 
  suppressMessages(suppressWarnings(missForest(covid.no.nan, verbose=FALSE)))

covid.full = covid.imp$ximp

#make sure these variables are factors
for (name in names(covid.full)) {
  if (is.factor(covid.full[,name])) {
    covid.full[,name] = droplevels(covid.full[,name])
    l = levels(droplevels(covid.full[,name]))
    if (length(l) < 2) {
      #drop factor columns with just one level
      covid.full = covid.full[,!(names(covid.full) %in% name)] 
    }
    
  }  
}




#dataset for processing
dataset = list(X = covid.full, y = pred.num) 


dim(dataset$X)


dataset$X$EDAD_ANO = as.numeric(dataset$X$EDAD_ANO)


#normalize age
print(mean(dataset$X$EDAD_ANO))
print(sd(dataset$X$EDAD_ANO))
dataset$X$EDAD_ANO = scale(dataset$X$EDAD_ANO)



#convert categorical variables into numeric
Xdummies = dummyVars( "~." , dataset$X)
transf = data.frame(predict(Xdummies, newdata = dataset$X))
dataset$X = transf




```
## XGBoost  Classifier

We built a XGBoost classifier

```{r include=FALSE}




sample.ind <- createDataPartition(dataset$y, p = 0.5, list = FALSE)

#construct a classifier

train.x = data.matrix(dataset$X[sample.ind,])
train.y = dataset$y[sample.ind]

test.x = data.matrix(dataset$X[-sample.ind,])
test.y = dataset$y[-sample.ind]

#https://insightr.wordpress.com/2018/05/17/tuning-xgboost-in-r-part-i/
# = parameters = #
# = eta candidates = #
eta=c(0.0001, 1)
# = colsample_bylevel candidates = #
cs=c(0.1,1)
# = max_depth candidates = #
md=c(1, 10)
# = sub_sample candidates = #
ss=c(0.1, 1)

# = min_child_weights candidates = #
mcw=c(1,400)

# = gamma candidates = #
gamma=c(0.01,100)

# = number of rounds candidates = #
rounds=c(10, 2000)


iter = 10000
old.AUC = 0

#this loop takes about eight hours!
for (i in seq(1,iter)) {
  
  #eta value
  r.eta = runif(1, eta[1], eta[2])
  #cs value
  r.cs = runif(1, cs[1], cs[2])
  
  # = max_depth candidates = #
  r.md= sample(md[1]:md[2],1)
  
  # = sub_sample candidates = #
  r.ss=runif(1, ss[1], ss[2])
  
  # = min_child_weights candidates = #
  r.mcw=sample(mcw[1]:mcw[2],1)
  
  # = gamma candidates = #
  r.gamma=runif(1, gamma[1], gamma[2])
  
  
  # = number of rounds candidates = #
  r.rounds=sample(rounds[1]:rounds[2],1)
  
  
  
  
  
  params=list(eta = r.eta, colsample_bylevel=r.cs,
              max_depth = r.md, subsample = r.ss, gamma = r.gamma)
  
  xgb <- suppressWarnings(suppressMessages(xgboost(data = train.x, 
                                                   label =train.y,nrounds = 500, 
                                                   verbose = 0, params = params)))
  y.pred = predict(xgb, test.x)
  
  minimo = -0.49
  maximo = 1.56

  y.pred.n = (y.pred - minimo)/(maximo - minimo)
  
  
  #performance results
  roc<-roc.curve(scores.class0 = y.pred.n,
                 weights.class0 = test.y, curve= TRUE)
  
  
  AUC = roc$auc
  if (AUC > old.AUC){
    old.AUC = AUC
    t.eta = r.eta
    t.cs = r.cs
    t.md = r.md
    t.ss = r.ss
    t.mcw = r.mcw
    
    t.gamma = r.gamma
    t.rounds = r.rounds
    
    print(c(i, AUC))
    print(t.eta) 
    print(t.cs) 
    print(t.md )
    print(t.ss) 
    print(t.mcw )
    print(t.gamma )
    print(t.rounds) 
  }
  
}

iter = 1000
old.AUC = 0

for (i in seq(1,iter)) {
  
  if ((i %% 100) == 0) {
    print(i)
  }
  # = number of rounds candidates = #
  r.rounds=sample(rounds[1]:rounds[2],1)
  
  
  
  r.eta = 0.1480863
  r.cs = 0.3408778
  r.md = 1
  r.ss = 0.452943
  r.gamma = 0.1494112
  
  params=list(eta = r.eta, colsample_bylevel=r.cs,
              max_depth = r.md, subsample = r.ss, gamma = r.gamma)
  
  xgb <- suppressWarnings(suppressMessages(xgboost(data = train.x, 
                                                   label =train.y,nrounds = r.rounds, 
                                                   verbose = 0, params = params)))
  y.pred = predict(xgb, test.x)
  
  minimo = -0.49
  maximo = 1.56

  y.pred.n = (y.pred - minimo)/(maximo - minimo)
  
  
  #performance results
  roc<-roc.curve(scores.class0 = y.pred.n,
                 weights.class0 = test.y, curve= TRUE)
  
  
  AUC = roc$auc
  if (AUC > old.AUC){
    old.AUC = AUC
    t.rounds = r.rounds
    
    print(c(i, AUC))
    print(t.rounds) 
  }
  
}





```


