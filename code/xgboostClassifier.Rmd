---
title: 'XGBoost classifier: IMSS dataset'
author: "joaquin salas"
date: "2020.06.29"
output:
  pdf_document: default
  word_document: default
  
  #data saved in: "featureSelection.RData"
  #save(list = ls(all.names=TRUE), file = "featureSelection.RData")
  
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
Given that a patient has been confirmed positive, 
this program construct an XGBoost classifier to infer whether the patient 
will improve or will die.
It uses the IMSS dataset at 
To assess the classifier, we construct ROC and precision-recall curves.



```{r  echo=FALSE}




#import libraries
suppressMessages(library(xgboost))

suppressMessages(library(caret))
suppressMessages(library(caTools))

suppressMessages(library(data.table)) # setnames

suppressMessages(require(dplyr))

suppressMessages(require(dummies)) #dummy variables
suppressMessages(require(ggplot2))
suppressMessages(require("Hmisc"))
suppressMessages(library("installr"))
suppressMessages(library(lubridate, warn.conflict = FALSE, quietly = TRUE))
suppressMessages(library(missForest)) #missForest
suppressMessages(library(matrixStats)) #colSds, standard deviation

suppressMessages(require(multiROC))
suppressMessages(require(nbpMatching))

suppressMessages(require(plotly))
suppressMessages(require(PRROC))
suppressMessages(library(pROC,verbose = TRUE )) #pROC, ROC analysis
suppressMessages(require(plyr))

suppressMessages(suppressWarnings(library(randomForest))) #for randomForest
suppressMessages(library(readr))

suppressMessages(library(stringi))

suppressMessages(library(tidyr)) #fill


suppressMessages(library(e1071))

suppressMessages(library(kernlab)) #svm



source("readDataSVM.R")




```


## Read the data

Dado que utilizaremos Boruta para la seleccion de caracteristicas, reducimos la base de datos mediante la siguiente estrategia. 


```{r preparacion, echo=FALSE}


#working directories
code.dir = 'E:/Documents/informs/research/2020.06.25 predicting death/code/'
setwd(code.dir)

#directories with files that  I use across many applications related to COVID
data.dir = 'E://Documents//informs//research//2020.06.25 predicting death//data//'



#files in the data.dir directory corresponding to the Health Ministery data
files.pattern = "*CENSO NOMINAL DF*.*"

files <- list.files(path = data.dir, pattern =  files.pattern)

#############

i = 1
for (file in files) {
  
  filename = paste(data.dir, file, sep = "")
  
  dataset = readDataSVM(filename = filename)
  
  if (i == 1) {
    data = dataset
  }
  else {
    data = rbind(data, dataset)
  }
  i = i + 1
}








covid = data[data$DIAGNOSTICO_FINAL == "COVID-19",!(names(data) %in% c("DIAGNOSTICO_FINAL"))]
condicion.medica = (covid$DESC_MOTIVO_EGRESO == "MEJORIA") | 
  (covid$DESC_MOTIVO_EGRESO == "DEFUNCION")

covid.diag = covid[condicion.medica,]

threshold.nan = 0 # the current database is full

covid.diag.pred = covid.diag[, !(names(covid.diag) %in% c("DESC_MOTIVO_EGRESO"))]

#change predictor from factor <MEJORIA, DEFUNCION> to factor <0,1>
pred = covid.diag$DESC_MOTIVO_EGRESO
pred.num = c(0,nrow=length(pred), ncol= 1)
pred.num[pred==c("MEJORIA")] = 0
pred.num[pred==c("DEFUNCION")] = 1


#pred.num = as.factor(pred.num)

too.many.nan = 
  names(covid.diag.pred)[colSums(is.na(covid.diag.pred)) > threshold.nan]


#remove variables for which there are too many missing values
covid.no.nan = covid.diag.pred[, !(names(covid.diag.pred) %in% c(too.many.nan))]




##fill missing values
covid.imp = 
  suppressMessages(suppressWarnings(missForest(covid.no.nan, verbose=FALSE)))

covid.full = covid.imp$ximp

#make sure these variables are factors
for (name in names(covid.full)) {
  if (is.factor(covid.full[,name])) {
    covid.full[,name] = droplevels(covid.full[,name])
    l = levels(droplevels(covid.full[,name]))
    if (length(l) < 2) {
      #drop factor columns with just one level
      covid.full = covid.full[,!(names(covid.full) %in% name)] 
    }
    
  }  
}




#dataset for processing
dataset = list(X = covid.full, y = pred.num) 


dim(dataset$X)


dataset$X$EDAD_ANO = as.numeric(dataset$X$EDAD_ANO)

#convert categorical variables into numeric
Xdummies = dummyVars( "~." , dataset$X)
transf = data.frame(predict(Xdummies, newdata = dataset$X))
dataset$X = transf


#normalize age
mu = mean(dataset$X$EDAD_ANO)
sigma = sd(dataset$X$EDAD_ANO)
print(mu)
print(sigma)
dataset$X$EDAD_ANO = scale(dataset$X$EDAD_ANO)





```
## Support Vector Machine Classifier

We built a Support Vector Machine classifier

```{r include=FALSE}

#X = dataset$X
#y = as.factor(dataset$y)


#ndata = cbind(X,y)

#names(ndata) = make.names(names(ndata))

#otherwise there will be an error during train
#levels(ndata$y)[1]<-"positive"
#levels(ndata$y)[2]<-"negative"


#seed for the random numbers generator
set.seed(123)


#perfom cross-validation cv.trial times


cv.trial = 30
#accumulate the results in the ROC and precision-recall curve
roc.auc = matrix(0,1, cv.trial)
pr.auc = matrix(0,1, cv.trial)


#divide subset datain half
smp.size = floor(0.5 * nrow(dataset$X))

minimo = 1e10
maximo = -1e10
#cross-validation iterations
for (trial in seq(1,cv.trial)) {
  
  print(trial)
  
  
  
  sample.ind <- createDataPartition(dataset$y, p = 0.5, list = FALSE)
  
  #construct a classifier
  
  train.x = data.matrix(dataset$X[sample.ind,])
  #train.y = as.numeric(dataset$y[sample.ind])-1
  train.y = dataset$y[sample.ind]
  
  test.x = data.matrix(dataset$X[-sample.ind,])
  #test.y = as.numeric(dataset$y[-sample.ind])-1
  test.y = dataset$y[-sample.ind]
  
  #construct a classifier
  
  
  params=list(eta = 0.1, colsample_bylevel=0.33,
              subsample = 0.5, max_depth = 10,
              min_child_weigth = 1,
              gamma = 0.1 ) 
  
  xgb <- xgboost(data = train.x, 
                 label =train.y,nrounds = 500, 
                 numclass = 2, verbose = 0, params = params)
  #ksvm.classifier = ksvm(y ~., data = ndata[ndata.ind,])
  
  
  
  #predict output. This is what you want to implement in an app
  y.pred = predict(xgb, test.x)
  
  #min.y = min(y.pred)
  #max.y = max(y.pred)
  #if (minimo > min.y){
  #  minimo = min.y
  #}
  #if (maximo < max.y){
  #  maximo = max.y
  #}
  
  
  minimo = -0.49
  maximo = 1.56

  y.pred.n = (y.pred - minimo)/(maximo - minimo)
  
  
  #performance results
  roc<-roc.curve(scores.class0 = y.pred.n,
                 weights.class0 = test.y, curve= TRUE)
  pr<-pr.curve(scores.class0 =  y.pred.n,
               weights.class0 = test.y, curve= TRUE)
  
  
  #save results for analysis
  roc.auc[ trial] = roc$auc
  pr.auc[ trial] = pr$auc.integral
  
  #save results for each classifier
  filename = sprintf("../data/xgb2_roc_%03d.csv", trial)
  #write.csv(roc$curve,filename)
  
  filename = sprintf("../data/xgb2_pr_%03d.csv", trial)
  #write.csv(pr$curve,filename)
  
  
  print(roc$auc)
  print(pr$auc.integral)
  #print(minimo)
  #print(maximo)
}








filename = paste(data.dir, "XGBoost.RData", sep = "")
save(list = ls(all.names=TRUE), file = filename)

lista = c("xgb", "minimo", "maximo","sigma", "mu", "dataset")
filename = paste(data.dir, "xgb_classifier.RData", sep = "")
save(list = lista, file = filename)

load(file = filename)

```


