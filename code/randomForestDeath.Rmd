---
title: 'Feature Selection: IMSS dataset'
author: "joaquin salas"
date: "2020.06.25"
output:
  pdf_document: default
  word_document: default
  
  #data saved in: "featureSelection.RData"
  #save(list = ls(all.names=TRUE), file = "featureSelection.RData")
  
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
This program construct a classifier to discriminate between those that live and those that die.
It uses the IMSS  dataset at https://datos.gob.mx/busca/dataset/informacion-referente-a-casos-covid-19-en-mexico
This program analyzes the features to confirm which ones are important. To assess the classifier, we construct ROC and precision-recall curves.



```{r  echo=FALSE}




#import libraries

suppressMessages(suppressWarnings(library(Boruta))) #for boruta, feature selection
suppressMessages(library(caret))
suppressMessages(library(caTools))

suppressMessages(library(data.table)) # setnames

suppressMessages(require(dplyr))

suppressMessages(require(dummies)) #dummy variables
suppressMessages(require(ggplot2))
suppressMessages(require("Hmisc"))
suppressMessages(library("installr"))
suppressMessages(library(lubridate, warn.conflict = FALSE, quietly = TRUE))
suppressMessages(library(missForest)) #missForest
suppressMessages(library(matrixStats)) #colSds, standard deviation

suppressMessages(require(multiROC))
suppressMessages(require(nbpMatching))

suppressMessages(require(plotly))
suppressMessages(require(PRROC))
suppressMessages(library(pROC,verbose = TRUE )) #pROC, ROC analysis
suppressMessages(require(plyr))

suppressMessages(suppressWarnings(library(randomForest))) #for randomForest
suppressMessages(library(readr))

suppressMessages(library(stringi))


suppressMessages(library(tidyr)) #fill



source("readData.R")
source("selectFeatures.R")




```


## Read the data

Dado que utilizaremos Boruta para la seleccion de caracteristicas, reducimos la base de datos mediante la siguiente estrategia. 


```{r preparacion, echo=FALSE}


 

```
## Random Forest Classifier

We built a Random Forest classifier with the predictors that were important in the Boruta analysis. 

```{r include=FALSE}


load(file = "featureSelectionDeath.RData")

X = dataset$X
X$EDAD_ANO = as.numeric(X$EDAD_ANO)
y = as.factor(dataset$y)


ndata = cbind(X,y)

#RandomForest has a limitation on the number of factors to 53
too.many.factors = c("MUNICIPIO_RES", "PAIS_NACIONALIDAD")
ndata = ndata[, !(names(ndata) %in% too.many.factors)]
selected = selected[,!(names(selected) %in% too.many.factors)]


selected[is.na(selected)] = FALSE #check why there are NA in the first place

#matrix with unique rows
unique.selected = unique.matrix(selected)

sum = colSums(unique.selected)
attributes.always = names(unique.selected)[sum == dim(unique.selected)[1]]

save.ndata = ndata
save.ndata$SEXO = as.numeric(save.ndata$SEXO)
save.ndata$SEXO[save.ndata$SEXO == 2] = 0
 

y = data.frame(y = save.ndata$y)

write.csv(cbind(save.ndata[,attributes.always], y),"../data/IMSS_CDMX.csv")

#unique classifiers
unique.rows = dim(unique.selected)[1]

#seed for the random numbers generator
set.seed(123)


#perfom cross-validation cv.trial times


cv.trial = 30
#accumulate the results in the ROC and precision-recall curve
roc.auc = matrix(0,unique.rows, cv.trial)
pr.auc = matrix(0,unique.rows, cv.trial)


#divide subset datain half
smp.size = floor(0.5 * nrow(ndata))


#for (num.classifier in c(1:unique.rows)) {

#  print(num.classifier)
  
  
  #column names that were selected for the classifier
#  attributes = colnames(unique.selected)[unique.selected[num.classifier,]>0]
  
  
  #cross-validation iterations
  for (trial in seq(1,cv.trial)) {
    
    print(trial)
    
  
    #split the dataset for training and testing
    train_ind <- sample(seq_len(nrow(ndata)), size = smp.size)
    
    train <- ndata[train_ind, ]
    test <- ndata[-train_ind, ]
    
    
    #construct a classifier
    rf_res <-randomForest(y = train$y  , x= train[, attributes.always], ntree=1000, keep.forest = TRUE)

    
    #predict output. This is what you want to implement in an app
    rf_pred = predict(rf_res,test[,attributes],type = "prob")
    
     #init an array with zeros
    ground.truth = array(0, dim = dim(rf_pred)[1] )
    ground.truth[test$y==1] = 1
    
    
    #performance results
    roc<-roc.curve(scores.class0 = rf_pred[,2], weights.class0 = ground.truth, curve= TRUE)
    pr<-pr.curve(scores.class0 =  rf_pred[,2], weights.class0 = ground.truth, curve= TRUE)
    
    
    #save results for analysis
    roc.auc[num.classifier, trial] = roc$auc
    pr.auc[num.classifier, trial] = pr$auc.integral
    
 
    #save results for each classifier
  filename = sprintf("../data/RF_roc_%03d.csv", trial)
  write.csv(roc$curve,filename)
  
  filename = sprintf("../data/RF_pr_%03d.csv", trial)
  write.csv(pr$curve,filename)
  

    print(roc$auc)
    print(pr$auc.integral)
    
  }
  
  
 
#}
  
  



#select one of the possible multiple classifiers
#minima = rowMins(roc.auc)

#maximum = max(minima)
#sel.classifier = which( minima == maximum)

#official = unique.selected[sel.classifier,]
#names(official)[official>0]



save(list = ls(all.names=TRUE), file = "featureSelectionSSA.RData")
load(file = "featureSelectionSSA.RData")

```


